---
# Evaluation Job - Runs LLM-as-a-Judge evaluation inside the cluster
#
# Usage:
#   kubectl apply -f manifests/13-evaluation-job.yaml
#   kubectl logs -n ai-agents job/eval-run -f
#   kubectl delete job eval-run -n ai-agents  # To re-run
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: eval-dataset
  namespace: ai-agents
data:
  dataset.json: |
    {
      "name": "enterprise-helpdesk-eval",
      "description": "Evaluation dataset for Enterprise AI Help Desk agents",
      "items": [
        {
          "id": "tech-001",
          "category": "tech-support",
          "input": "My VPN keeps disconnecting every few minutes",
          "expected_agent": "tech-support",
          "expected_topics": ["vpn", "connectivity", "troubleshooting"],
          "quality_criteria": "Should provide step-by-step troubleshooting steps for VPN issues"
        },
        {
          "id": "tech-002",
          "category": "tech-support",
          "input": "I forgot my password and cannot log into my laptop",
          "expected_agent": "tech-support",
          "expected_topics": ["password", "reset", "access"],
          "quality_criteria": "Should guide user to password reset process or SSO portal"
        },
        {
          "id": "hr-001",
          "category": "hr",
          "input": "How many vacation days do I get after 3 years at the company?",
          "expected_agent": "hr",
          "expected_topics": ["pto", "vacation", "policy"],
          "quality_criteria": "Should correctly state 20 days for 3-5 years tenure",
          "expected_answer_contains": "20"
        },
        {
          "id": "hr-002",
          "category": "hr",
          "input": "What is the 401k match policy?",
          "expected_agent": "hr",
          "expected_topics": ["401k", "benefits", "retirement"],
          "quality_criteria": "Should mention 4% match",
          "expected_answer_contains": "4%"
        },
        {
          "id": "hr-003",
          "category": "hr",
          "input": "How many sick days do I have per year?",
          "expected_agent": "hr",
          "expected_topics": ["sick leave", "health", "policy"],
          "quality_criteria": "Should correctly state 10 sick days per year",
          "expected_answer_contains": "10"
        },
        {
          "id": "kb-001",
          "category": "knowledge-base",
          "input": "Where is the New York office located?",
          "expected_agent": "knowledge-base",
          "expected_topics": ["office", "location", "address"],
          "quality_criteria": "Should provide correct address: 200 Park Avenue",
          "expected_answer_contains": "200 Park Avenue"
        },
        {
          "id": "kb-002",
          "category": "knowledge-base",
          "input": "How do I submit an expense report?",
          "expected_agent": "knowledge-base",
          "expected_topics": ["expense", "report", "process"],
          "quality_criteria": "Should mention Concur and 30-day deadline",
          "expected_answer_contains": "Concur"
        },
        {
          "id": "kb-003",
          "category": "knowledge-base",
          "input": "Who is the CEO of the company?",
          "expected_agent": "knowledge-base",
          "expected_topics": ["ceo", "leadership", "executive"],
          "quality_criteria": "Should correctly identify Jane Smith as CEO",
          "expected_answer_contains": "Jane Smith"
        }
      ]
    }
---
apiVersion: batch/v1
kind: Job
metadata:
  name: eval-run
  namespace: ai-agents
  labels:
    app: evaluation
spec:
  ttlSecondsAfterFinished: 3600
  template:
    metadata:
      labels:
        app: evaluation
    spec:
      restartPolicy: Never
      containers:
      - name: evaluator
        image: python:3.11-slim
        env:
        - name: DEMO_URL
          value: "http://demo-orchestrator.ai-agents.svc.cluster.local"
        - name: AZURE_PROXY_URL
          value: "http://azure-proxy.ai-agents.svc.cluster.local/v1/chat/completions"
        - name: LANGFUSE_PUBLIC_KEY
          valueFrom:
            secretKeyRef:
              name: langfuse-secrets
              key: langfuse-public-key
        - name: LANGFUSE_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: langfuse-secrets
              key: langfuse-secret-key
        - name: LANGFUSE_HOST
          valueFrom:
            secretKeyRef:
              name: langfuse-secrets
              key: langfuse-host
        volumeMounts:
        - name: dataset
          mountPath: /data
        command: ["python", "-c"]
        args:
        - |
          import subprocess
          subprocess.run(["pip", "install", "langfuse==2.44.0", "-q"])

          import os
          import json
          import time
          import urllib.request
          from datetime import datetime

          from langfuse import Langfuse

          # Initialize
          langfuse = Langfuse(
              public_key=os.environ.get('LANGFUSE_PUBLIC_KEY'),
              secret_key=os.environ.get('LANGFUSE_SECRET_KEY'),
              host=os.environ.get('LANGFUSE_HOST', 'https://cloud.langfuse.com')
          )

          DEMO_URL = os.environ.get('DEMO_URL')
          AZURE_PROXY = os.environ.get('AZURE_PROXY_URL')

          # LLM-as-a-Judge prompts
          ROUTING_JUDGE = """You are evaluating an AI routing system.

          User Query: {input}
          Expected Agent: {expected_agent}
          Actual Agent: {actual_agent}

          Score from 0-1:
          - 1.0: Correct routing
          - 0.5: Reasonable alternative
          - 0.0: Wrong routing

          Respond with JSON only: {{"score": <number>, "reasoning": "<explanation>"}}"""

          QUALITY_JUDGE = """You are evaluating an AI response quality.

          User Query: {input}
          Response: {response}
          Criteria: {criteria}
          Expected Content: {expected}

          Evaluate: relevance, accuracy, helpfulness, professionalism.
          Score from 0-1.

          Respond with JSON only: {{"score": <number>, "reasoning": "<explanation>"}}"""

          def call_llm_judge(prompt):
              """Call Azure OpenAI for evaluation"""
              try:
                  data = json.dumps({
                      "model": "gpt-4o",
                      "messages": [{"role": "user", "content": prompt}],
                      "max_tokens": 200,
                      "temperature": 0
                  }).encode()
                  req = urllib.request.Request(AZURE_PROXY, data=data, headers={'Content-Type': 'application/json'})
                  with urllib.request.urlopen(req, timeout=60) as resp:
                      result = json.loads(resp.read().decode())
                      content = result['choices'][0]['message']['content']
                      # Parse JSON
                      if '```' in content:
                          content = content.split('```')[1].replace('json', '').strip()
                      return json.loads(content)
              except Exception as e:
                  return {"score": 0, "reasoning": f"Error: {str(e)}"}

          def call_demo(message):
              """Call demo endpoint"""
              try:
                  data = json.dumps({"message": message}).encode()
                  req = urllib.request.Request(f"{DEMO_URL}/ask", data=data, headers={'Content-Type': 'application/json'})
                  with urllib.request.urlopen(req, timeout=120) as resp:
                      return json.loads(resp.read().decode())
              except Exception as e:
                  return {"error": str(e)}

          # Load dataset
          with open('/data/dataset.json', 'r') as f:
              dataset = json.load(f)

          run_name = f"k8s-eval-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
          print(f"\n{'='*60}")
          print(f"  AgentGateway Evaluation: {run_name}")
          print(f"  Items: {len(dataset['items'])}")
          print(f"{'='*60}\n")

          results = []

          for i, item in enumerate(dataset['items']):
              print(f"[{i+1}/{len(dataset['items'])}] {item['id']}: {item['input'][:40]}...")

              # Create trace
              trace = langfuse.trace(
                  name="evaluation",
                  metadata={"run": run_name, "item": item['id'], "category": item['category']},
                  tags=["evaluation", run_name, item['category']]
              )

              try:
                  # Call demo
                  start = time.time()
                  result = call_demo(item['input'])
                  latency = time.time() - start

                  actual_agent = result.get('routed_to', {}).get('agent', 'unknown')
                  response = result.get('response', '')

                  print(f"    Routed: {actual_agent} | Latency: {latency:.2f}s")

                  # Evaluate routing
                  if item['expected_agent'] != 'any':
                      routing_eval = call_llm_judge(ROUTING_JUDGE.format(
                          input=item['input'],
                          expected_agent=item['expected_agent'],
                          actual_agent=actual_agent
                      ))
                      langfuse.score(trace_id=trace.id, name="routing", value=routing_eval.get('score', 0))
                      print(f"    Routing: {routing_eval.get('score', 0):.2f}")

                  # Evaluate quality
                  expected = item.get('expected_answer_contains', 'N/A')
                  if isinstance(expected, list): expected = ', '.join(expected)

                  quality_eval = call_llm_judge(QUALITY_JUDGE.format(
                      input=item['input'],
                      response=response[:500],
                      criteria=item.get('quality_criteria', ''),
                      expected=expected
                  ))
                  langfuse.score(trace_id=trace.id, name="quality", value=quality_eval.get('score', 0))
                  print(f"    Quality: {quality_eval.get('score', 0):.2f}")

                  # Overall score
                  scores = [routing_eval.get('score', 0) if item['expected_agent'] != 'any' else 1,
                            quality_eval.get('score', 0)]
                  overall = sum(scores) / len(scores)
                  langfuse.score(trace_id=trace.id, name="overall", value=overall)

                  results.append({
                      "id": item['id'],
                      "routing": routing_eval.get('score', 0) if item['expected_agent'] != 'any' else None,
                      "quality": quality_eval.get('score', 0),
                      "overall": overall,
                      "latency": latency
                  })

              except Exception as e:
                  print(f"    ERROR: {str(e)}")
                  results.append({"id": item['id'], "error": str(e)})

              langfuse.flush()
              print()

          # Summary
          successful = [r for r in results if 'error' not in r]
          print(f"\n{'='*60}")
          print(f"  SUMMARY")
          print(f"{'='*60}")
          print(f"  Total: {len(results)} | Success: {len(successful)} | Failed: {len(results)-len(successful)}")

          if successful:
              avg_overall = sum(r['overall'] for r in successful) / len(successful)
              avg_quality = sum(r['quality'] for r in successful) / len(successful)
              routing_scores = [r['routing'] for r in successful if r['routing'] is not None]
              avg_routing = sum(routing_scores) / len(routing_scores) if routing_scores else 0
              avg_latency = sum(r['latency'] for r in successful) / len(successful)

              print(f"\n  Scores:")
              print(f"    Overall:  {avg_overall:.2%}")
              print(f"    Routing:  {avg_routing:.2%}")
              print(f"    Quality:  {avg_quality:.2%}")
              print(f"    Latency:  {avg_latency:.2f}s")

          print(f"\n  View in Langfuse: https://cloud.langfuse.com")
          print(f"  Filter by tag: {run_name}")
          print(f"{'='*60}\n")

      volumes:
      - name: dataset
        configMap:
          name: eval-dataset
