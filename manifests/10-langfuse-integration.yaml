---
# Langfuse Secrets
# To use: kubectl create secret generic langfuse-secrets --namespace ai-agents \
#   --from-literal=langfuse-public-key="pk-..." \
#   --from-literal=langfuse-secret-key="sk-..." \
#   --from-literal=langfuse-host="https://cloud.langfuse.com"
apiVersion: v1
kind: Secret
metadata:
  name: langfuse-secrets
  namespace: ai-agents
type: Opaque
stringData:
  langfuse-public-key: "pk-placeholder"
  langfuse-secret-key: "sk-placeholder"
  langfuse-host: "https://cloud.langfuse.com"
---
# Azure OpenAI Proxy with Langfuse Tracing
apiVersion: apps/v1
kind: Deployment
metadata:
  name: azure-proxy-langfuse
  namespace: ai-agents
  labels:
    app: azure-proxy-langfuse
spec:
  replicas: 1
  selector:
    matchLabels:
      app: azure-proxy-langfuse
  template:
    metadata:
      labels:
        app: azure-proxy-langfuse
    spec:
      containers:
      - name: azure-proxy
        image: python:3.11-slim
        command: ["python3", "-c"]
        args:
        - |
          import subprocess
          subprocess.run(["pip", "install", "flask", "requests", "langfuse==2.44.0", "-q"])

          from flask import Flask, request, Response, jsonify
          import os
          import json
          import uuid
          from datetime import datetime
          from langfuse import Langfuse
          import requests as req

          app = Flask(__name__)

          # Azure OpenAI config
          AZURE_ENDPOINT = os.environ.get('AZURE_ENDPOINT', '')
          API_KEY = os.environ.get('AZURE_API_KEY', '')
          DEPLOYMENT = os.environ.get('AZURE_DEPLOYMENT', 'gpt-4o')

          # Langfuse config
          langfuse = Langfuse(
              public_key=os.environ.get('LANGFUSE_PUBLIC_KEY'),
              secret_key=os.environ.get('LANGFUSE_SECRET_KEY'),
              host=os.environ.get('LANGFUSE_HOST', 'https://cloud.langfuse.com')
          )

          @app.route('/health')
          def health():
              return {"status": "healthy", "langfuse": "enabled"}

          @app.route('/v1/chat/completions', methods=['POST'])
          def chat():
              data = request.json
              messages = data.get('messages', [])
              trace_id = str(uuid.uuid4())

              # Create Langfuse trace
              trace = langfuse.trace(
                  id=trace_id,
                  name="azure-openai-chat",
                  metadata={
                      "model": DEPLOYMENT,
                      "provider": "azure-openai",
                      "gateway": "kgateway"
                  },
                  tags=["kgateway", "azure", "proxy"]
              )

              # Create generation
              generation = langfuse.generation(
                  trace_id=trace_id,
                  name="chat-completion",
                  model=DEPLOYMENT,
                  input=messages,
                  model_parameters={
                      "temperature": data.get('temperature', 1.0),
                      "max_tokens": data.get('max_tokens'),
                  }
              )

              try:
                  url = f"{AZURE_ENDPOINT}/openai/deployments/{DEPLOYMENT}/chat/completions?api-version=2024-10-21"
                  resp = req.post(url, json=data, headers={"Content-Type": "application/json", "api-key": API_KEY})
                  response_data = resp.json()

                  # Extract output for Langfuse
                  output = None
                  usage = None
                  if 'choices' in response_data:
                      output = response_data['choices'][0].get('message', {})
                  if 'usage' in response_data:
                      usage = response_data['usage']

                  # Update generation with output
                  generation.end(
                      output=output,
                      usage={
                          "input": usage.get('prompt_tokens') if usage else None,
                          "output": usage.get('completion_tokens') if usage else None,
                      } if usage else None
                  )

                  return Response(resp.content, status=resp.status_code, content_type='application/json')

              except Exception as e:
                  generation.end(
                      output={"error": str(e)},
                      level="ERROR",
                      status_message=str(e)
                  )
                  return jsonify({"error": str(e)}), 500
              finally:
                  langfuse.flush()

          @app.route('/v1/models')
          def models():
              return {"object": "list", "data": [{"id": DEPLOYMENT, "object": "model", "owned_by": "azure"}]}

          print("Starting Azure Proxy with Langfuse tracing on port 8080...")
          app.run(host='0.0.0.0', port=8080)
        env:
        - name: AZURE_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: llm-secrets
              key: azure-endpoint
        - name: AZURE_API_KEY
          valueFrom:
            secretKeyRef:
              name: llm-secrets
              key: azure-api-key
        - name: AZURE_DEPLOYMENT
          valueFrom:
            secretKeyRef:
              name: llm-secrets
              key: azure-deployment
              optional: true
        - name: LANGFUSE_PUBLIC_KEY
          valueFrom:
            secretKeyRef:
              name: langfuse-secrets
              key: langfuse-public-key
        - name: LANGFUSE_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: langfuse-secrets
              key: langfuse-secret-key
        - name: LANGFUSE_HOST
          valueFrom:
            secretKeyRef:
              name: langfuse-secrets
              key: langfuse-host
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "300m"
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 45
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: azure-proxy-langfuse
  namespace: ai-agents
  labels:
    app: azure-proxy-langfuse
spec:
  selector:
    app: azure-proxy-langfuse
  ports:
  - port: 80
    targetPort: 8080
---
# A2A Orchestrator with Langfuse Tracing
apiVersion: apps/v1
kind: Deployment
metadata:
  name: a2a-orchestrator-langfuse
  namespace: ai-agents
  labels:
    app: a2a-orchestrator-langfuse
spec:
  replicas: 1
  selector:
    matchLabels:
      app: a2a-orchestrator-langfuse
  template:
    metadata:
      labels:
        app: a2a-orchestrator-langfuse
    spec:
      containers:
      - name: orchestrator
        image: python:3.11-slim
        ports:
        - containerPort: 8080
        env:
        - name: RESEARCH_AGENT_URL
          value: "http://a2a-research-agent.ai-agents.svc.cluster.local:9090"
        - name: CODING_AGENT_URL
          value: "http://a2a-coding-agent.ai-agents.svc.cluster.local:9090"
        - name: SUPPORT_AGENT_URL
          value: "http://a2a-support-agent.ai-agents.svc.cluster.local:9090"
        - name: LANGFUSE_PUBLIC_KEY
          valueFrom:
            secretKeyRef:
              name: langfuse-secrets
              key: langfuse-public-key
        - name: LANGFUSE_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: langfuse-secrets
              key: langfuse-secret-key
        - name: LANGFUSE_HOST
          valueFrom:
            secretKeyRef:
              name: langfuse-secrets
              key: langfuse-host
        command: ["python", "-c"]
        args:
        - |
          import subprocess
          subprocess.run(["pip", "install", "langfuse==2.44.0", "-q"])

          from http.server import HTTPServer, BaseHTTPRequestHandler
          import json
          import urllib.request
          import os
          import uuid
          from langfuse import Langfuse

          RESEARCH_AGENT = os.environ.get('RESEARCH_AGENT_URL')
          CODING_AGENT = os.environ.get('CODING_AGENT_URL')
          SUPPORT_AGENT = os.environ.get('SUPPORT_AGENT_URL')

          # Initialize Langfuse
          langfuse = Langfuse(
              public_key=os.environ.get('LANGFUSE_PUBLIC_KEY'),
              secret_key=os.environ.get('LANGFUSE_SECRET_KEY'),
              host=os.environ.get('LANGFUSE_HOST', 'https://cloud.langfuse.com')
          )

          def call_a2a_agent(agent_url, message, trace_id):
              """Call an A2A agent and return its response"""
              # Create span for agent call
              span = langfuse.span(
                  trace_id=trace_id,
                  name=f"a2a-agent-call",
                  input={"url": agent_url, "message": message}
              )

              try:
                  request_data = {
                      "id": "task-from-orchestrator",
                      "sessionId": "orchestrator-session",
                      "message": {
                          "role": "user",
                          "parts": [{"type": "text", "text": message}]
                      }
                  }
                  req = urllib.request.Request(
                      f"{agent_url}/tasks/send",
                      data=json.dumps(request_data).encode(),
                      headers={'Content-Type': 'application/json'},
                      method='POST'
                  )

                  with urllib.request.urlopen(req, timeout=30) as response:
                      result = json.loads(response.read().decode())
                      span.end(output=result)
                      return result
              except Exception as e:
                  span.end(output={"error": str(e)}, level="ERROR")
                  return {"error": str(e)}

          def get_agent_card(agent_url):
              """Get agent card from A2A agent"""
              try:
                  req = urllib.request.Request(f"{agent_url}/.well-known/agent.json")
                  with urllib.request.urlopen(req, timeout=10) as response:
                      return json.loads(response.read().decode())
              except Exception as e:
                  return {"error": str(e)}

          class OrchestratorHandler(BaseHTTPRequestHandler):
              def log_message(self, format, *args):
                  print(f"[Orchestrator-Langfuse] {args[0]}")

              def send_json(self, data, status=200):
                  self.send_response(status)
                  self.send_header('Content-Type', 'application/json')
                  self.send_header('Access-Control-Allow-Origin', '*')
                  self.end_headers()
                  self.wfile.write(json.dumps(data, indent=2).encode())

              def do_OPTIONS(self):
                  self.send_response(200)
                  self.send_header('Access-Control-Allow-Origin', '*')
                  self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
                  self.send_header('Access-Control-Allow-Headers', 'Content-Type')
                  self.end_headers()

              def do_GET(self):
                  if self.path == '/agents':
                      agents = {
                          "agents": [
                              {"name": "research", "url": RESEARCH_AGENT, "card": get_agent_card(RESEARCH_AGENT)},
                              {"name": "coding", "url": CODING_AGENT, "card": get_agent_card(CODING_AGENT)},
                              {"name": "support", "url": SUPPORT_AGENT, "card": get_agent_card(SUPPORT_AGENT)}
                          ]
                      }
                      self.send_json(agents)
                  elif self.path == '/health':
                      self.send_json({"status": "healthy", "service": "orchestrator-langfuse"})
                  else:
                      self.send_json({
                          "service": "A2A Orchestrator with Langfuse",
                          "endpoints": {
                              "GET /agents": "List all available A2A agents",
                              "POST /delegate": "Delegate task to specific agent",
                              "POST /orchestrate": "Let orchestrator choose best agent"
                          }
                      })

              def do_POST(self):
                  content_length = int(self.headers.get('Content-Length', 0))
                  body = self.rfile.read(content_length).decode()

                  try:
                      request = json.loads(body) if body else {}
                  except:
                      self.send_json({"error": "Invalid JSON"}, 400)
                      return

                  if self.path == '/orchestrate':
                      message = request.get('message', '').lower()
                      original_message = request.get('message', '')
                      trace_id = str(uuid.uuid4())

                      # Create Langfuse trace for orchestration
                      trace = langfuse.trace(
                          id=trace_id,
                          name="a2a-orchestration",
                          input={"message": original_message},
                          metadata={"gateway": "kgateway", "protocol": "a2a"},
                          tags=["a2a", "orchestrator", "kgateway"]
                      )

                      # Create routing span
                      routing_span = langfuse.span(
                          trace_id=trace_id,
                          name="route-decision",
                          input={"message": message}
                      )

                      # Simple routing logic
                      if any(word in message for word in ['code', 'program', 'function', 'bug', 'review', 'script', 'debug']):
                          agent = 'coding'
                          agent_url = CODING_AGENT
                      elif any(word in message for word in ['help', 'support', 'issue', 'problem', 'complaint', 'refund', 'cancel', 'account', 'billing', 'password', 'subscription', 'customer']):
                          agent = 'support'
                          agent_url = SUPPORT_AGENT
                      else:
                          agent = 'research'
                          agent_url = RESEARCH_AGENT

                      routing_span.end(output={"selected_agent": agent, "agent_url": agent_url})

                      # Call agent with tracing
                      result = call_a2a_agent(agent_url, original_message, trace_id)

                      response = {
                          "orchestrated_to": agent,
                          "reasoning": f"Routed to {agent} agent based on message content",
                          "message": original_message,
                          "response": result,
                          "trace_id": trace_id
                      }

                      langfuse.flush()
                      self.send_json(response)

                  elif self.path == '/delegate':
                      agent = request.get('agent', 'research')
                      message = request.get('message', '')
                      trace_id = str(uuid.uuid4())

                      trace = langfuse.trace(
                          id=trace_id,
                          name="a2a-delegation",
                          input={"agent": agent, "message": message},
                          tags=["a2a", "delegation"]
                      )

                      agent_url = RESEARCH_AGENT if agent == 'research' else (CODING_AGENT if agent == 'coding' else SUPPORT_AGENT)
                      result = call_a2a_agent(agent_url, message, trace_id)

                      response = {"delegated_to": agent, "message": message, "response": result, "trace_id": trace_id}
                      langfuse.flush()

                      self.send_json(response)
                  else:
                      self.send_json({"error": "Unknown endpoint"}, 404)

          print("Starting A2A Orchestrator with Langfuse on port 8080...")
          HTTPServer(('', 8080), OrchestratorHandler).serve_forever()
---
apiVersion: v1
kind: Service
metadata:
  name: a2a-orchestrator-langfuse
  namespace: ai-agents
spec:
  selector:
    app: a2a-orchestrator-langfuse
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
---
# Backend for Azure Proxy with Langfuse
apiVersion: gateway.kgateway.dev/v1alpha1
kind: Backend
metadata:
  name: azure-langfuse-backend
  namespace: kgateway-system
spec:
  type: Static
  static:
    hosts:
    - host: azure-proxy-langfuse.ai-agents.svc.cluster.local
      port: 80
---
# Backend for A2A Orchestrator with Langfuse
apiVersion: gateway.kgateway.dev/v1alpha1
kind: Backend
metadata:
  name: a2a-orchestrator-langfuse-backend
  namespace: kgateway-system
spec:
  type: Static
  static:
    hosts:
    - host: a2a-orchestrator-langfuse.ai-agents.svc.cluster.local
      port: 80
---
# Azure OpenAI with Langfuse Route
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: azure-langfuse-route
  namespace: kgateway-system
spec:
  parentRefs:
  - name: agentgateway
    namespace: kgateway-system
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /azure-traced
    filters:
    - type: URLRewrite
      urlRewrite:
        path:
          type: ReplacePrefixMatch
          replacePrefixMatch: /v1/chat/completions
    backendRefs:
    - name: azure-langfuse-backend
      namespace: kgateway-system
      group: gateway.kgateway.dev
      kind: Backend
---
# A2A Orchestrator with Langfuse Route
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: a2a-orchestrator-langfuse-route
  namespace: kgateway-system
spec:
  parentRefs:
  - name: agentgateway
    namespace: kgateway-system
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /a2a-traced
    filters:
    - type: URLRewrite
      urlRewrite:
        path:
          type: ReplacePrefixMatch
          replacePrefixMatch: /
    backendRefs:
    - name: a2a-orchestrator-langfuse-backend
      namespace: kgateway-system
      group: gateway.kgateway.dev
      kind: Backend
